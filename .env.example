# Required: Target website URL to scrape
WEBSITE_URL=http://localhost:8000

# Optional: Ollama configuration
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=codellama:13b

# Optional: Server port
PORT=8080

# Optional: URL pattern restrictions for scraping
# Comma-separated list of URL patterns that are allowed for scraping
# If not set, all URLs are allowed
# Examples:
# - Allow only localhost: ALLOWED_SCRAPING_URL_PATTERNS=localhost
# - Allow specific directories: ALLOWED_SCRAPING_URL_PATTERNS=github.com/username,linkedin.com/in/,example.com/api/
# - Allow multiple sites and paths: ALLOWED_SCRAPING_URL_PATTERNS=localhost,github.com,linkedin.com/in/,stackoverflow.com/users/
# - Allow only profiles: ALLOWED_SCRAPING_URL_PATTERNS=github.com/,linkedin.com/in/,gitlab.com/
ALLOWED_SCRAPING_URL_PATTERNS=localhost,github.com,linkedin.com/in/,gitlab.com,stackoverflow.com/users/,medium.com/@,dev.to/

# Enable scraping of internal navigation links (not just external professional links)
# Set to "true" for e-commerce sites, documentation sites, or multi-page websites
# Set to "false" or omit for personal portfolio sites that only need external profile scraping
ENABLE_INTERNAL_LINK_SCRAPING=false

# Content caching behavior
# Set to "true" to force refresh of scraped content on every request
# Set to "false" or omit to use cached content from disk for 24 hours (recommended for speed)
# Cached content is stored in scraped_content/ directory per website URL
REFRESH_CONTENT=false

# Minimum text length for content scraping
# Text fragments shorter than this will be filtered out during scraping
# Higher values reduce noise but may miss short important content
# Lower values capture more content but may include irrelevant fragments
MIN_TEXT_LENGTH=3

# Maximum text length for individual text fragments during scraping
# Text fragments longer than this will be filtered out during scraping
# Higher values capture more detailed content but may include verbose text
# Lower values focus on concise content but may miss detailed information
MAX_CONTENT_LENGTH=10000

# Maximum scraping depth for recursive link following
# 1 = only main page, 2 = main + direct links, 3+ = recursive scraping
# Higher values get more comprehensive data but take much longer
# Range: 1-10, recommended: 2-5 depending on site complexity
MAX_SCRAPING_DEPTH=2

# Maximum pages to scrape per session (safety limit)
# Prevents runaway scraping from consuming too many resources
# Adjust based on target website size and server capacity
MAX_PAGES_PER_SESSION=100

MAX_TOTAL_CONTENT_LENGTH=10000